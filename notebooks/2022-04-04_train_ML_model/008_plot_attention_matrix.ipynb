{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Iterable, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (18, 10)\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "# ML imports\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import sklearn.manifold\n",
    "\n",
    "# power_perceiver imports\n",
    "from power_perceiver.load_prepared_batches.prepared_dataset import PreparedDataset\n",
    "from power_perceiver.consts import BatchKey\n",
    "from power_perceiver.load_prepared_batches.data_loader import HRVSatellite, PV, Sun\n",
    "from power_perceiver.xr_batch_processor import SelectPVSystemsNearCenterOfImage, ReduceNumPVSystems, ReduceNumTimesteps\n",
    "from power_perceiver.np_batch_processor import EncodeSpaceTime, Topography\n",
    "from power_perceiver.transforms.satellite import PatchSatellite\n",
    "from power_perceiver.transforms.pv import PVPowerRollingWindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH = Path(\"~/dev/ocf/power_perceiver/data_for_testing/\").expanduser()\n",
    "\n",
    "DATA_PATH = Path(\n",
    "    \"/mnt/storage_ssd_4tb/data/ocf/solar_pv_nowcasting/nowcasting_dataset_pipeline/prepared_ML_training_data/v15/\")\n",
    "assert DATA_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyresample/image.py:59: FutureWarning: Usage of ImageContainer is deprecated, please use NumpyResamplerBilinear class instead\n",
      "  warnings.warn(\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyproj/crs/crs.py:1256: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n",
      "  return self._crs.to_proj4(version=version)\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyproj/crs/crs.py:1256: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n",
      "  return self._crs.to_proj4(version=version)\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyresample/image.py:59: FutureWarning: Usage of ImageContainer is deprecated, please use NumpyResamplerBilinear class instead\n",
      "  warnings.warn(\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyresample/image.py:59: FutureWarning: Usage of ImageContainer is deprecated, please use NumpyResamplerBilinear class instead\n",
      "  warnings.warn(\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyproj/crs/crs.py:1256: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n",
      "  return self._crs.to_proj4(version=version)\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyproj/crs/crs.py:1256: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n",
      "  return self._crs.to_proj4(version=version)\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyresample/image.py:59: FutureWarning: Usage of ImageContainer is deprecated, please use NumpyResamplerBilinear class instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_dataloader(data_path: Path, tag: str) -> data.DataLoader:\n",
    "    assert tag in [\"train\", \"validation\"]\n",
    "    assert data_path.exists()\n",
    "    \n",
    "    xr_batch_processors = [\n",
    "        SelectPVSystemsNearCenterOfImage(),\n",
    "        ReduceNumPVSystems(requested_num_pv_systems=8),\n",
    "        ]\n",
    "    \n",
    "    if tag == \"train\":\n",
    "        xr_batch_processors.append(ReduceNumTimesteps(requested_timesteps=4))\n",
    "    \n",
    "    dataset = PreparedDataset(\n",
    "        data_path=data_path,\n",
    "        data_loaders=[\n",
    "            HRVSatellite(\n",
    "                transforms=[PatchSatellite()]\n",
    "                ), \n",
    "            PV(\n",
    "                transforms=[PVPowerRollingWindow()]\n",
    "                ),\n",
    "            Sun(),\n",
    "        ],\n",
    "        xr_batch_processors=xr_batch_processors,\n",
    "        np_batch_processors=[\n",
    "            EncodeSpaceTime(),\n",
    "            Topography(\"/home/jack/europe_dem_2km_osgb.tif\"),\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    dataloader = data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=None,\n",
    "        num_workers=16,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "#train_dataloader = get_dataloader(DATA_PATH)\n",
    "train_dataloader = get_dataloader(DATA_PATH / \"train\", tag=\"train\")\n",
    "val_dataloader = get_dataloader(DATA_PATH / \"test\", tag=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 4, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[BatchKey.pv].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[BatchKey.pv_time_utc].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[BatchKey.pv].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from power_perceiver.pytorch_modules.satellite_processor import HRVSatelliteProcessor\n",
    "from power_perceiver.pytorch_modules.query_generator import QueryGenerator\n",
    "from power_perceiver.pytorch_modules.self_attention import MultiLayerTransformerEncoder\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(eq=False)  # See https://discuss.pytorch.org/t/typeerror-unhashable-type-for-my-torch-nn-module/109424/6\n",
    "class Model(pl.LightningModule):\n",
    "    # Params for Perceiver\n",
    "    query_dim: int = 36  # byte_array and query will be automatically padded with zeros to get to this size.\n",
    "    num_fourier_features: int = 16 # TOTAL for both x and y\n",
    "    pv_system_id_embedding_dim: int = 16\n",
    "    num_heads: int = 6\n",
    "    dropout: float = 0.0\n",
    "    share_weights_across_latent_transformer_layers: bool = False\n",
    "    num_latent_transformer_encoders: int = 4\n",
    "    \n",
    "    # Other params:\n",
    "    num_elements_query_padding: int = 0  # Probably keep this at zero while using MultiLayerTransformerEncoder\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__init__()\n",
    "        self.hrvsatellite_processor = HRVSatelliteProcessor()\n",
    "        \n",
    "        self.query_generator = QueryGenerator(\n",
    "            num_fourier_features=self.num_fourier_features,  # TOTAL (for both x and y)\n",
    "            pv_system_id_embedding_dim=self.pv_system_id_embedding_dim,\n",
    "            num_elements_query_padding=self.num_elements_query_padding)\n",
    "               \n",
    "        self.transformer_encoder = MultiLayerTransformerEncoder(\n",
    "            d_model=self.query_dim, \n",
    "            num_heads=self.num_heads,\n",
    "            dropout=self.dropout,\n",
    "            share_weights_across_latent_transformer_layers=self.share_weights_across_latent_transformer_layers,\n",
    "            num_latent_transformer_encoders=self.num_latent_transformer_encoders,\n",
    "            )\n",
    "        \n",
    "        \"\"\"\n",
    "        self.perceiver = Perceiver(\n",
    "            query_dim=self.query_dim,\n",
    "            byte_array_dim=self.byte_array_dim,\n",
    "            num_heads=self.num_heads,\n",
    "            dropout=self.dropout,\n",
    "            share_weights_across_latent_transformer_layers=self.share_weights_across_latent_transformer_layers,\n",
    "            num_latent_transformer_encoders=self.num_latent_transformer_encoders,\n",
    "            )\n",
    "        \"\"\"\n",
    "        \n",
    "        self.output_module = nn.Sequential(\n",
    "            nn.Linear(in_features=self.query_dim, out_features=self.query_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.query_dim, out_features=1),\n",
    "        )\n",
    "\n",
    "        # Do this at the end of __post_init__ to capture model topology:\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def forward(self, x: dict[BatchKey, torch.Tensor]) -> torch.Tensor:       \n",
    "        original_batch_size = x[BatchKey.pv].shape[0]\n",
    "        byte_array = self.hrvsatellite_processor(x)\n",
    "        query = self.query_generator(x)\n",
    "        \n",
    "        # Pad with zeros if necessary to get up to self.query_dim:\n",
    "        byte_array = self._maybe_pad_with_zeros(byte_array)\n",
    "        query = self._maybe_pad_with_zeros(query)            \n",
    "        \n",
    "        # Prepare the attention input and run through the transformer_encoder:\n",
    "        attn_input = torch.concat((byte_array, query), dim=1)            \n",
    "        attn_output = self.transformer_encoder(attn_input)\n",
    "\n",
    "        # Select the elements of the output which correspond to the query:\n",
    "        out = attn_output[:, byte_array.shape[1]:]\n",
    "        \n",
    "        out = self.output_module(out)\n",
    "        \n",
    "        # Reshape back to (batch_size, n_timesteps, ...)\n",
    "        return einops.rearrange(\n",
    "            out, \n",
    "            \"(batch_size n_timesteps) ... -> batch_size n_timesteps ...\", \n",
    "            batch_size=original_batch_size)\n",
    "        \n",
    "    def _maybe_pad_with_zeros(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        num_zeros_to_pad = self.query_dim - tensor.shape[-1]        \n",
    "        assert num_zeros_to_pad >= 0, f\"{self.query_dim=}, {tensor.shape=}\"\n",
    "        if num_zeros_to_pad > 0:\n",
    "            zero_padding_shape = tensor.shape[:2] + (num_zeros_to_pad,)\n",
    "            zero_padding = torch.zeros(*zero_padding_shape, dtype=tensor.dtype, device=tensor.device)\n",
    "            tensor = torch.concat((tensor, zero_padding), dim=2)\n",
    "        return tensor\n",
    "    \n",
    "    def _training_or_validation_step(\n",
    "            self, \n",
    "            batch: dict[BatchKey, torch.Tensor], \n",
    "            batch_idx: int, \n",
    "            tag: str\n",
    "        ) -> dict[str, object]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: The training or validation batch.  A dictionary.\n",
    "            tag: Either \"train\" or \"validation\"\n",
    "            batch_idx: The index of the batch.\n",
    "        \"\"\"\n",
    "        actual_pv_power = batch[BatchKey.pv]\n",
    "        #actual_pv_power = torch.nan_to_num(actual_pv_power, nan=0.0)\n",
    "        actual_pv_power = torch.where(\n",
    "            batch[BatchKey.pv_mask].unsqueeze(1), \n",
    "            actual_pv_power, \n",
    "            torch.tensor(0.0, dtype=actual_pv_power.dtype, device=actual_pv_power.device))\n",
    "\n",
    "        predicted_pv_power = self(batch).squeeze()\n",
    "        #mse_loss = F.mse_loss(predicted_pv_power, actual_pv_power, reduction=\"none\").mean(dim=1).float()\n",
    "        #mse_loss = masked_mean(mse_loss, mask=batch[BatchKey.pv_mask])        \n",
    "        mse_loss = F.mse_loss(predicted_pv_power, actual_pv_power)\n",
    "        \n",
    "        self.log(f\"{tag}/mse\", mse_loss)\n",
    "        \n",
    "        return {\n",
    "            'loss': mse_loss,\n",
    "            'predicted_pv_power': predicted_pv_power,\n",
    "            }\n",
    "    \n",
    "    def training_step(self, batch: dict[BatchKey, torch.Tensor], batch_idx: int) -> dict[str, object]:\n",
    "        return self._training_or_validation_step(batch=batch, batch_idx=batch_idx, tag=\"train\")\n",
    "    \n",
    "    def validation_step(self, batch: dict[BatchKey, torch.Tensor], batch_idx: int) -> dict[str, object]:\n",
    "        return self._training_or_validation_step(batch=batch, batch_idx=batch_idx, tag=\"validation\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "#model = Model.load_from_checkpoint(\n",
    "#    \"~/dev/ocf/power_perceiver/notebooks/2022-04-04_train_ML_model/model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "\n",
    "\n",
    "def forward_pre_hook(module, args) -> tuple:\n",
    "    \"\"\"A simple hook to set `need_weights` to True.\"\"\"\n",
    "    query, key, value, key_padding_mask, need_weights, attn_mask = args\n",
    "    need_weights = True\n",
    "    return query, key, value, key_padding_mask, need_weights, attn_mask\n",
    "\n",
    "attn_weights = {}\n",
    "def get_attn_weights(name: str) -> Callable:\n",
    "    # Adapted from https://web.stanford.edu/~nanbhas/blog/forward-hooks-pytorch/\n",
    "    def forward_hook(module, input, output):\n",
    "        attn_output, attn_output_weights = output\n",
    "        attn_weights[name] = attn_output_weights\n",
    "    return forward_hook\n",
    "\n",
    "pre_hook = model.transformer_encoder.transformer_encoder.layers[0].self_attn.register_forward_pre_hook(forward_pre_hook)\n",
    "f_hook = model.transformer_encoder.transformer_encoder.layers[0].self_attn.register_forward_hook(\n",
    "    get_attn_weights(name=\"layer0\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.transformer_encoder.transformer_encoder.layers[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(batch)\n",
    "model_output.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_hook.remove()\n",
    "f_hook.remove()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attn_weights[\"layer0\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[BatchKey.hrvsatellite_x_osgb].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cartopy.crs as ccrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_IDX = 11\n",
    "TIMESTEP_IDX = 0\n",
    "PV_SYSTEM_IDX = 3\n",
    "\n",
    "projection = ccrs.OSGB(approx=False)\n",
    "\n",
    "nrows = 1\n",
    "ncols = 3\n",
    "shape = (nrows, ncols)\n",
    "\n",
    "ax1 = plt.subplot2grid(shape, loc=(0, 0), projection=projection)\n",
    "ax2 = plt.subplot2grid(shape, loc=(0, 1), projection=projection)\n",
    "ax3 = plt.subplot2grid(shape, loc=(0, 2))\n",
    "\n",
    "ax1.set_title(\"Attention\")\n",
    "ax1.pcolormesh(\n",
    "    batch[BatchKey.hrvsatellite_x_osgb][BATCH_IDX],\n",
    "    batch[BatchKey.hrvsatellite_y_osgb][BATCH_IDX],\n",
    "    attn_weights[\"layer0\"][(4*BATCH_IDX)+TIMESTEP_IDX, 256+PV_SYSTEM_IDX][:256].detach().numpy().reshape(16, 16),\n",
    "    )\n",
    "\n",
    "date = pd.to_datetime(batch[BatchKey.hrvsatellite_time_utc][BATCH_IDX, TIMESTEP_IDX], unit=\"s\")\n",
    "\n",
    "ax2.set_title(f\"Satellite {date}\")\n",
    "\n",
    "ax2.pcolormesh(\n",
    "    batch[BatchKey.hrvsatellite_x_osgb][BATCH_IDX].numpy().repeat(4, axis=0).repeat(4, axis=1),\n",
    "    batch[BatchKey.hrvsatellite_y_osgb][BATCH_IDX].numpy().repeat(4, axis=0).repeat(4, axis=1),\n",
    "    einops.rearrange(\n",
    "        batch[BatchKey.hrvsatellite][BATCH_IDX, TIMESTEP_IDX, 0], \n",
    "        \"y x (patch_size_y patch_size_x) -> (y patch_size_y) (x patch_size_x)\",\n",
    "        patch_size_y=4,\n",
    "        patch_size_x=4\n",
    "        ),\n",
    ")\n",
    "\n",
    "ax3.set_title(\"Satellite unprojected\")\n",
    "ax3.imshow(\n",
    "    einops.rearrange(\n",
    "        batch[BatchKey.hrvsatellite][BATCH_IDX, TIMESTEP_IDX, 0], \n",
    "        \"y x (patch_size_y patch_size_x) -> (y patch_size_y) (x patch_size_x)\",\n",
    "        patch_size_y=4,\n",
    "        patch_size_x=4\n",
    "        ),\n",
    "    extent=(\n",
    "        batch[BatchKey.hrvsatellite_x_osgb][BATCH_IDX][-1, 0],\n",
    "        batch[BatchKey.hrvsatellite_x_osgb][BATCH_IDX][0, -1],\n",
    "        batch[BatchKey.hrvsatellite_y_osgb][BATCH_IDX][0, -1],\n",
    "        batch[BatchKey.hrvsatellite_y_osgb][BATCH_IDX][-1, 0],\n",
    "    ), # left, right, bottom, top\n",
    "    origin=\"lower\",\n",
    ")\n",
    "\n",
    "\n",
    "for ax in (ax1, ax2, ax3):\n",
    "    ax.scatter(\n",
    "        batch[BatchKey.pv_x_osgb][BATCH_IDX, PV_SYSTEM_IDX],\n",
    "        batch[BatchKey.pv_y_osgb][BATCH_IDX, PV_SYSTEM_IDX],\n",
    "        color=\"white\",\n",
    "    )\n",
    "    \n",
    "for ax in (ax1, ax2):\n",
    "    ylim = ax.get_ylim()\n",
    "    xlim = ax.get_xlim()\n",
    "    BORDER_METERS = 50_000\n",
    "    ax.set_ylim(ylim[0]-BORDER_METERS, ylim[1]+BORDER_METERS)\n",
    "    ax.set_xlim(xlim[0]-BORDER_METERS, xlim[1]+BORDER_METERS)\n",
    "\n",
    "    ax.coastlines()\n",
    "\n",
    "# TODO:\n",
    "# Plot with OSGB coords\n",
    "# Overlay coastline so we can see what's what\n",
    "# Plot location of PV system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    project=\"power_perceiver\", \n",
    "    entity=\"openclimatefix\",\n",
    "    log_model=\"all\",\n",
    "    )\n",
    "\n",
    "# log gradients, parameter histogram and model topology\n",
    "wandb_logger.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    gpus=[3],\n",
    "    max_epochs=-1,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        LogTimeseriesPlots(),\n",
    "        LogTSNEPlot(),\n",
    "    ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(\n",
    "    model=model, \n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e018056da7ac22974c0de454ad66e6f150c494c7b54611c2027650045c28434"
  },
  "kernelspec": {
   "display_name": "power_perceiver",
   "language": "python",
   "name": "power_perceiver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
