{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# General imports\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional, Iterable, Any\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (18, 10)\n",
    "plt.rcParams[\"figure.facecolor\"] = \"white\"\n",
    "\n",
    "# ML imports\n",
    "import torch\n",
    "from torch.utils import data\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import einops\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import wandb\n",
    "import sklearn.manifold\n",
    "\n",
    "# power_perceiver imports\n",
    "from power_perceiver.dataset import NowcastingDataset\n",
    "from power_perceiver.consts import BatchKey\n",
    "from power_perceiver.data_loader import HRVSatellite, PV, Sun\n",
    "from power_perceiver.xr_batch_processor import SelectPVSystemsNearCenterOfImage, ReduceNumPVSystems, ReduceNumTimesteps\n",
    "from power_perceiver.np_batch_processor import EncodeSpaceTime, Topography\n",
    "from power_perceiver.transforms.satellite import PatchSatellite\n",
    "from power_perceiver.transforms.pv import PVPowerRollingWindow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_PATH = Path(\"~/dev/ocf/power_perceiver/data_for_testing/\").expanduser()\n",
    "\n",
    "DATA_PATH = Path(\n",
    "    \"/mnt/storage_ssd_4tb/data/ocf/solar_pv_nowcasting/nowcasting_dataset_pipeline/prepared_ML_training_data/v15/\")\n",
    "assert DATA_PATH.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyresample/image.py:59: FutureWarning: Usage of ImageContainer is deprecated, please use NumpyResamplerBilinear class instead\n",
      "  warnings.warn(\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyproj/crs/crs.py:1256: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n",
      "  return self._crs.to_proj4(version=version)\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyproj/crs/crs.py:1256: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n",
      "  return self._crs.to_proj4(version=version)\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyresample/image.py:59: FutureWarning: Usage of ImageContainer is deprecated, please use NumpyResamplerBilinear class instead\n",
      "  warnings.warn(\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyresample/image.py:59: FutureWarning: Usage of ImageContainer is deprecated, please use NumpyResamplerBilinear class instead\n",
      "  warnings.warn(\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyproj/crs/crs.py:1256: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n",
      "  return self._crs.to_proj4(version=version)\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyproj/crs/crs.py:1256: UserWarning: You will likely lose important projection information when converting to a PROJ string from another format. See: https://proj.org/faq.html#what-is-the-best-format-for-describing-coordinate-reference-systems\n",
      "  return self._crs.to_proj4(version=version)\n",
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pyresample/image.py:59: FutureWarning: Usage of ImageContainer is deprecated, please use NumpyResamplerBilinear class instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def get_dataloader(data_path: Path, tag: str) -> data.DataLoader:\n",
    "    assert tag in [\"train\", \"validation\"]\n",
    "    assert data_path.exists()\n",
    "    \n",
    "    xr_batch_processors = [\n",
    "        SelectPVSystemsNearCenterOfImage(),\n",
    "        ReduceNumPVSystems(requested_num_pv_systems=8),\n",
    "        ]\n",
    "    \n",
    "    if tag == \"train\":\n",
    "        xr_batch_processors.append(ReduceNumTimesteps(requested_timesteps=4))\n",
    "    \n",
    "    dataset = NowcastingDataset(\n",
    "        data_path=data_path,\n",
    "        data_loaders=[\n",
    "            HRVSatellite(\n",
    "                transforms=[PatchSatellite()]\n",
    "                ), \n",
    "            PV(\n",
    "                transforms=[PVPowerRollingWindow()]\n",
    "                ),\n",
    "            Sun(),\n",
    "        ],\n",
    "        xr_batch_processors=xr_batch_processors,\n",
    "        np_batch_processors=[\n",
    "            EncodeSpaceTime(),\n",
    "            Topography(\"/home/jack/europe_dem_2km_osgb.tif\"),\n",
    "            ]\n",
    "    )\n",
    "\n",
    "    dataloader = data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=None,\n",
    "        num_workers=16,\n",
    "        pin_memory=True,\n",
    "    )\n",
    "    \n",
    "    return dataloader\n",
    "\n",
    "#train_dataloader = get_dataloader(DATA_PATH)\n",
    "train_dataloader = get_dataloader(DATA_PATH / \"train\", tag=\"train\")\n",
    "val_dataloader = get_dataloader(DATA_PATH / \"test\", tag=\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_dataloader:\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 4, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[BatchKey.pv].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 4])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[BatchKey.pv_time_utc].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([31, 4, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[BatchKey.pv].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from power_perceiver.pytorch_modules.satellite_processor import HRVSatelliteProcessor\n",
    "from power_perceiver.pytorch_modules.query_generator import QueryGenerator\n",
    "from power_perceiver.pytorch_modules.self_attention import PerceiverIO\n",
    "\n",
    "\n",
    "@dataclass(eq=False)  # See https://discuss.pytorch.org/t/typeerror-unhashable-type-for-my-torch-nn-module/109424/6\n",
    "class Model(pl.LightningModule):\n",
    "    encoder_query_dim: int = 64\n",
    "    num_encoder_query_elements: int = 64\n",
    "    decoder_query_dim: int = 36  # decoder_query will be automatically padded with zeros to get to this size.\n",
    "    num_fourier_features: int = 16 # TOTAL for both x and y\n",
    "    pv_system_id_embedding_dim: int = 16\n",
    "    byte_array_dim: int = 35\n",
    "    num_encoder_heads: int = 8\n",
    "    num_decoder_heads: int = 6\n",
    "    dropout: float = 0.0\n",
    "    share_weights_across_latent_transformer_layers: bool = False\n",
    "    num_latent_transformer_encoders: int = 4\n",
    "    \n",
    "    # Other params:\n",
    "    num_elements_query_padding: int = 0  # Probably keep this at zero while using MultiLayerTransformerEncoder or Perceiver IO\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__init__()\n",
    "        self.hrvsatellite_processor = HRVSatelliteProcessor()\n",
    "        \n",
    "        self.encoder_query = nn.Parameter(\n",
    "            torch.randn(self.num_encoder_query_elements, self.encoder_query_dim) / 5)\n",
    "        \n",
    "        self.decoder_query_generator = QueryGenerator(\n",
    "            num_fourier_features=self.num_fourier_features,  # TOTAL (for both x and y)\n",
    "            pv_system_id_embedding_dim=self.pv_system_id_embedding_dim,\n",
    "            num_elements_query_padding=self.num_elements_query_padding)\n",
    "        \n",
    "        self.perceiver_io = PerceiverIO(\n",
    "            encoder_query_dim=self.encoder_query_dim,\n",
    "            decoder_query_dim=self.decoder_query_dim,\n",
    "            byte_array_dim=self.byte_array_dim,\n",
    "            num_encoder_heads=self.num_encoder_heads,\n",
    "            num_decoder_heads=self.num_decoder_heads,\n",
    "            dropout=self.dropout,\n",
    "            share_weights_across_latent_transformer_layers=self.share_weights_across_latent_transformer_layers,\n",
    "            num_latent_transformer_encoders=self.num_latent_transformer_encoders,\n",
    "            )\n",
    "\n",
    "        self.output_module = nn.Sequential(\n",
    "            nn.Linear(in_features=self.decoder_query_dim, out_features=self.decoder_query_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(in_features=self.decoder_query_dim, out_features=1),\n",
    "        )\n",
    "\n",
    "        # Do this at the end of __post_init__ to capture model topology to wandb:\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "    def forward(self, x: dict[BatchKey, torch.Tensor]) -> torch.Tensor:       \n",
    "        original_batch_size = x[BatchKey.pv].shape[0]\n",
    "        byte_array = self.hrvsatellite_processor(x)\n",
    "        \n",
    "        # Get decoder query\n",
    "        decoder_query = self.decoder_query_generator(x)\n",
    "        # Pad with zeros if necessary to get up to self.decoder_query_dim:\n",
    "        decoder_query = self._maybe_pad_with_zeros(decoder_query)            \n",
    "        \n",
    "        # Repeat encoder query for each example in the batch:\n",
    "        encoder_query = einops.repeat(\n",
    "            self.encoder_query, \n",
    "            \"element feature -> example element feature\",\n",
    "            example=byte_array.shape[0],\n",
    "            )\n",
    "        \n",
    "        # Run through the Perceiver IO:\n",
    "        out = self.perceiver_io(\n",
    "            encoder_query=encoder_query,\n",
    "            byte_array=byte_array,\n",
    "            decoder_query=decoder_query,\n",
    "            )\n",
    "        \n",
    "        out = self.output_module(out)\n",
    "        \n",
    "        # Reshape back to (batch_size, n_timesteps, ...)\n",
    "        return einops.rearrange(\n",
    "            out, \n",
    "            \"(batch_size n_timesteps) ... -> batch_size n_timesteps ...\", \n",
    "            batch_size=original_batch_size)\n",
    "        \n",
    "    def _maybe_pad_with_zeros(self, tensor: torch.Tensor) -> torch.Tensor:\n",
    "        num_zeros_to_pad = self.decoder_query_dim - tensor.shape[-1]        \n",
    "        assert num_zeros_to_pad >= 0, f\"{self.query_dim=}, {tensor.shape=}\"\n",
    "        if num_zeros_to_pad > 0:\n",
    "            zero_padding_shape = tensor.shape[:2] + (num_zeros_to_pad,)\n",
    "            zero_padding = torch.zeros(*zero_padding_shape, dtype=tensor.dtype, device=tensor.device)\n",
    "            tensor = torch.concat((tensor, zero_padding), dim=2)\n",
    "        return tensor\n",
    "    \n",
    "    def _training_or_validation_step(\n",
    "            self, \n",
    "            batch: dict[BatchKey, torch.Tensor], \n",
    "            batch_idx: int, \n",
    "            tag: str\n",
    "        ) -> dict[str, object]:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            batch: The training or validation batch.  A dictionary.\n",
    "            tag: Either \"train\" or \"validation\"\n",
    "            batch_idx: The index of the batch.\n",
    "        \"\"\"\n",
    "        actual_pv_power = batch[BatchKey.pv]\n",
    "        #actual_pv_power = torch.nan_to_num(actual_pv_power, nan=0.0)\n",
    "        actual_pv_power = torch.where(\n",
    "            batch[BatchKey.pv_mask].unsqueeze(1), \n",
    "            actual_pv_power, \n",
    "            torch.tensor(0.0, dtype=actual_pv_power.dtype, device=actual_pv_power.device))\n",
    "\n",
    "        predicted_pv_power = self(batch).squeeze()\n",
    "        #mse_loss = F.mse_loss(predicted_pv_power, actual_pv_power, reduction=\"none\").mean(dim=1).float()\n",
    "        #mse_loss = masked_mean(mse_loss, mask=batch[BatchKey.pv_mask])        \n",
    "        mse_loss = F.mse_loss(predicted_pv_power, actual_pv_power)\n",
    "        \n",
    "        self.log(f\"{tag}/mse\", mse_loss)\n",
    "        \n",
    "        return {\n",
    "            'loss': mse_loss,\n",
    "            'predicted_pv_power': predicted_pv_power,\n",
    "            }\n",
    "    \n",
    "    def training_step(self, batch: dict[BatchKey, torch.Tensor], batch_idx: int) -> dict[str, object]:\n",
    "        return self._training_or_validation_step(batch=batch, batch_idx=batch_idx, tag=\"train\")\n",
    "    \n",
    "    def validation_step(self, batch: dict[BatchKey, torch.Tensor], batch_idx: int) -> dict[str, object]:\n",
    "        return self._training_or_validation_step(batch=batch, batch_idx=batch_idx, tag=\"validation\")\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=1e-4)\n",
    "        return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model()\n",
    "#model = Model.load_from_checkpoint(\n",
    "#    \"~/dev/ocf/power_perceiver/notebooks/2022-04-04_train_ML_model/model.ckpt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_output = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mopenclimatefix\u001b[0m (use `wandb login --relogin` to force relogin)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.12.14 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.12.1<br/>\n",
       "                Syncing run <strong style=\"color:#cdcd00\">feasible-wood-28</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/openclimatefix/power_perceiver\" target=\"_blank\">https://wandb.ai/openclimatefix/power_perceiver</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/openclimatefix/power_perceiver/runs/5iv2yzi9\" target=\"_blank\">https://wandb.ai/openclimatefix/power_perceiver/runs/5iv2yzi9</a><br/>\n",
       "                Run data is saved locally in <code>/home/jack/dev/ocf/power_perceiver/notebooks/2022-04-04_train_ML_model/wandb/run-20220411_143540-5iv2yzi9</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: logging graph, to disable use `wandb.watch(log_graph=False)`\n"
     ]
    }
   ],
   "source": [
    "wandb_logger = WandbLogger(\n",
    "    project=\"power_perceiver\", \n",
    "    entity=\"openclimatefix\",\n",
    "    log_model=\"all\",\n",
    "    )\n",
    "\n",
    "# log gradients, parameter histogram and model topology\n",
    "wandb_logger.watch(model, log=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "from power_perceiver.analysis.plot_timeseries import LogTimeseriesPlots\n",
    "from power_perceiver.analysis.plot_tsne import LogTSNEPlot\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=[3],\n",
    "    max_epochs=-1,\n",
    "    logger=wandb_logger,\n",
    "    callbacks=[\n",
    "        LogTimeseriesPlots(),\n",
    "        LogTSNEPlot(),\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/configuration_validator.py:342: LightningDeprecationWarning: Base `Callback.on_train_batch_end` hook signature has changed in v1.5. The `dataloader_idx` argument will be removed in v1.7.\n",
      "  rank_zero_deprecation(\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5]\n",
      "\n",
      "  | Name                    | Type                  | Params\n",
      "------------------------------------------------------------------\n",
      "0 | hrvsatellite_processor  | HRVSatelliteProcessor | 0     \n",
      "1 | decoder_query_generator | QueryGenerator        | 32.0 K\n",
      "2 | perceiver_io            | PerceiverIO           | 1.1 M \n",
      "3 | output_module           | Sequential            | 1.4 K \n",
      "------------------------------------------------------------------\n",
      "1.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.2 M     Total params\n",
      "4.729     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|                                                             | 0/2 [00:01<?, ?it/s]"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Embedding' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/home/jack/dev/ocf/power_perceiver/notebooks/2022-04-04_train_ML_model/007_perceiver_IO.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/jack/dev/ocf/power_perceiver/notebooks/2022-04-04_train_ML_model/007_perceiver_IO.ipynb#ch0000024?line=0'>1</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jack/dev/ocf/power_perceiver/notebooks/2022-04-04_train_ML_model/007_perceiver_IO.ipynb#ch0000024?line=1'>2</a>\u001b[0m     model\u001b[39m=\u001b[39;49mmodel, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jack/dev/ocf/power_perceiver/notebooks/2022-04-04_train_ML_model/007_perceiver_IO.ipynb#ch0000024?line=2'>3</a>\u001b[0m     train_dataloaders\u001b[39m=\u001b[39;49mtrain_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jack/dev/ocf/power_perceiver/notebooks/2022-04-04_train_ML_model/007_perceiver_IO.ipynb#ch0000024?line=3'>4</a>\u001b[0m     val_dataloaders\u001b[39m=\u001b[39;49mval_dataloader,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/jack/dev/ocf/power_perceiver/notebooks/2022-04-04_train_ML_model/007_perceiver_IO.ipynb#ch0000024?line=4'>5</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:771\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=751'>752</a>\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=752'>753</a>\u001b[0m \u001b[39mRuns the full optimization routine.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=753'>754</a>\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=767'>768</a>\u001b[0m \u001b[39m    datamodule: An instance of :class:`~pytorch_lightning.core.datamodule.LightningDataModule`.\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=768'>769</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=769'>770</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m model\n\u001b[0;32m--> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=770'>771</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_and_handle_interrupt(\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=771'>772</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fit_impl, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=772'>773</a>\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:724\u001b[0m, in \u001b[0;36mTrainer._call_and_handle_interrupt\u001b[0;34m(self, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=721'>722</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstrategy\u001b[39m.\u001b[39mlauncher\u001b[39m.\u001b[39mlaunch(trainer_fn, \u001b[39m*\u001b[39margs, trainer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=722'>723</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=723'>724</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m trainer_fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=724'>725</a>\u001b[0m \u001b[39m# TODO: treat KeyboardInterrupt as BaseException (delete the code below) in v1.7\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=725'>726</a>\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m \u001b[39mas\u001b[39;00m exception:\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:812\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=807'>808</a>\u001b[0m ckpt_path \u001b[39m=\u001b[39m ckpt_path \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresume_from_checkpoint\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=808'>809</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ckpt_path \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m__set_ckpt_path(\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=809'>810</a>\u001b[0m     ckpt_path, model_provided\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, model_connected\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlightning_module \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=810'>811</a>\u001b[0m )\n\u001b[0;32m--> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=811'>812</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run(model, ckpt_path\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mckpt_path)\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=813'>814</a>\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mstopped\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=814'>815</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1237\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1232'>1233</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mrestore_training_state()\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1234'>1235</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_checkpoint_connector\u001b[39m.\u001b[39mresume_end()\n\u001b[0;32m-> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1236'>1237</a>\u001b[0m results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_stage()\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1238'>1239</a>\u001b[0m log\u001b[39m.\u001b[39mdetail(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m: trainer tearing down\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1239'>1240</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_teardown()\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1324\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1321'>1322</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredicting:\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1322'>1323</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_run_predict()\n\u001b[0;32m-> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1323'>1324</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_train()\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1346\u001b[0m, in \u001b[0;36mTrainer._run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1342'>1343</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pre_training_routine()\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1344'>1345</a>\u001b[0m \u001b[39mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1345'>1346</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_sanity_check()\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1347'>1348</a>\u001b[0m \u001b[39m# enable train mode\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1348'>1349</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mtrain()\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1414\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1411'>1412</a>\u001b[0m \u001b[39m# run eval step\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1412'>1413</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m-> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1413'>1414</a>\u001b[0m     val_loop\u001b[39m.\u001b[39;49mrun()\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1415'>1416</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_callback_hooks(\u001b[39m\"\u001b[39m\u001b[39mon_sanity_check_end\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1417'>1418</a>\u001b[0m \u001b[39m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py:153\u001b[0m, in \u001b[0;36mEvaluationLoop.advance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_dataloaders \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=151'>152</a>\u001b[0m     kwargs[\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m dataloader_idx\n\u001b[0;32m--> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=152'>153</a>\u001b[0m dl_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepoch_loop\u001b[39m.\u001b[39;49mrun(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data_fetcher, dl_max_batches, kwargs)\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=154'>155</a>\u001b[0m \u001b[39m# store batch level output per dataloader\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/dataloader/evaluation_loop.py?line=155'>156</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_outputs\u001b[39m.\u001b[39mappend(dl_outputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/base.py:204\u001b[0m, in \u001b[0;36mLoop.run\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=201'>202</a>\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=202'>203</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_start(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=203'>204</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49madvance(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=204'>205</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mon_advance_end()\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/base.py?line=205'>206</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_restarting \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:133\u001b[0m, in \u001b[0;36mEvaluationEpochLoop.advance\u001b[0;34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=129'>130</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_processed()\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=131'>132</a>\u001b[0m \u001b[39m# track loss history\u001b[39;00m\n\u001b[0;32m--> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=132'>133</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_evaluation_batch_end(output, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=134'>135</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbatch_progress\u001b[39m.\u001b[39mincrement_completed()\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=136'>137</a>\u001b[0m \u001b[39m# log batch metrics\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py:263\u001b[0m, in \u001b[0;36mEvaluationEpochLoop._on_evaluation_batch_end\u001b[0;34m(self, output, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=260'>261</a>\u001b[0m kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mdataloader_idx\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m0\u001b[39m)  \u001b[39m# TODO: the argument should be keyword for these\u001b[39;00m\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=261'>262</a>\u001b[0m hook_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mon_test_batch_end\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39mtesting \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mon_validation_batch_end\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=262'>263</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainer\u001b[39m.\u001b[39;49m_call_callback_hooks(hook_name, output, \u001b[39m*\u001b[39;49mkwargs\u001b[39m.\u001b[39;49mvalues())\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=263'>264</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_call_lightning_module_hook(hook_name, output, \u001b[39m*\u001b[39mkwargs\u001b[39m.\u001b[39mvalues())\n\u001b[1;32m    <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/loops/epoch/evaluation_epoch_loop.py?line=265'>266</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainer\u001b[39m.\u001b[39m_logger_connector\u001b[39m.\u001b[39mon_batch_end()\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py:1637\u001b[0m, in \u001b[0;36mTrainer._call_callback_hooks\u001b[0;34m(self, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1634'>1635</a>\u001b[0m         \u001b[39mif\u001b[39;00m callable(fn):\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1635'>1636</a>\u001b[0m             \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mprofile(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m[Callback]\u001b[39m\u001b[39m{\u001b[39;00mcallback\u001b[39m.\u001b[39mstate_key\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m{\u001b[39;00mhook_name\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m):\n\u001b[0;32m-> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1636'>1637</a>\u001b[0m                 fn(\u001b[39mself\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlightning_module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1638'>1639</a>\u001b[0m \u001b[39mif\u001b[39;00m pl_module:\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1639'>1640</a>\u001b[0m     \u001b[39m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/pytorch_lightning/trainer/trainer.py?line=1640'>1641</a>\u001b[0m     pl_module\u001b[39m.\u001b[39m_current_fx_name \u001b[39m=\u001b[39m prev_fx_name\n",
      "File \u001b[0;32m~/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py:56\u001b[0m, in \u001b[0;36mSimpleCallback.on_validation_batch_end\u001b[0;34m(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_validation_batch_end\u001b[39m(\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=47'>48</a>\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=48'>49</a>\u001b[0m     trainer: pl\u001b[39m.\u001b[39mTrainer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=53'>54</a>\u001b[0m     dataloader_idx: \u001b[39mint\u001b[39m,\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=54'>55</a>\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=55'>56</a>\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_on_batch_end(\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=56'>57</a>\u001b[0m         trainer\u001b[39m=\u001b[39;49mtrainer,\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=57'>58</a>\u001b[0m         pl_module\u001b[39m=\u001b[39;49mpl_module,\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=58'>59</a>\u001b[0m         outputs\u001b[39m=\u001b[39;49moutputs,\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=59'>60</a>\u001b[0m         batch\u001b[39m=\u001b[39;49mbatch,\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=60'>61</a>\u001b[0m         batch_idx\u001b[39m=\u001b[39;49mbatch_idx,\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=61'>62</a>\u001b[0m         dataloader_idx\u001b[39m=\u001b[39;49mdataloader_idx,\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=62'>63</a>\u001b[0m         tag\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mvalidation\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/simple_callback.py?line=63'>64</a>\u001b[0m     )\n",
      "File \u001b[0;32m~/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py:72\u001b[0m, in \u001b[0;36mLogTSNEPlot._on_batch_end\u001b[0;34m(self, trainer, pl_module, outputs, batch, batch_idx, dataloader_idx, tag)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=69'>70</a>\u001b[0m \u001b[39mif\u001b[39;00m batch_idx \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=70'>71</a>\u001b[0m     query_generator \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(pl_module, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquery_generator_name)\n\u001b[0;32m---> <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=71'>72</a>\u001b[0m     fig \u001b[39m=\u001b[39m plot_tsne_of_pv_system_id_embedding(batch, query_generator\u001b[39m.\u001b[39;49mpv_system_id_embedding)\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=72'>73</a>\u001b[0m     wandb\u001b[39m.\u001b[39mlog(\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=73'>74</a>\u001b[0m         {\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=74'>75</a>\u001b[0m             \u001b[39m# Need to convert to image to avoid bug in matplotlib to plotly conversion\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=77'>78</a>\u001b[0m         },\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=78'>79</a>\u001b[0m     )\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=79'>80</a>\u001b[0m     plt\u001b[39m.\u001b[39mclose(fig)\n",
      "File \u001b[0;32m~/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py:33\u001b[0m, in \u001b[0;36mplot_tsne_of_pv_system_id_embedding\u001b[0;34m(batch, pv_system_id_embedding)\u001b[0m\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=27'>28</a>\u001b[0m     pv_system_row_numbers_for_all_examples\u001b[39m.\u001b[39mextend(row_numbers_for_example)\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=29'>30</a>\u001b[0m pv_system_row_numbers_for_all_examples \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39munique(pv_system_row_numbers_for_all_examples)\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=30'>31</a>\u001b[0m pv_system_row_numbers_for_all_examples \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfrom_numpy(\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=31'>32</a>\u001b[0m     pv_system_row_numbers_for_all_examples\n\u001b[0;32m---> <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=32'>33</a>\u001b[0m )\u001b[39m.\u001b[39mto(device\u001b[39m=\u001b[39mpv_system_id_embedding\u001b[39m.\u001b[39;49mdevice)\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=33'>34</a>\u001b[0m pv_id_embedding \u001b[39m=\u001b[39m pv_system_id_embedding(pv_system_row_numbers_for_all_examples)\n\u001b[1;32m     <a href='file:///home/jack/dev/ocf/power_perceiver/power_perceiver/analysis/plot_tsne.py?line=34'>35</a>\u001b[0m pv_id_embedding \u001b[39m=\u001b[39m pv_id_embedding\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mcpu()\n",
      "File \u001b[0;32m~/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/torch/nn/modules/module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1182'>1183</a>\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1183'>1184</a>\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1184'>1185</a>\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   <a href='file:///home/jack/miniconda3/envs/power_perceiver/lib/python3.10/site-packages/torch/nn/modules/module.py?line=1185'>1186</a>\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Embedding' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "trainer.fit(\n",
    "    model=model, \n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e018056da7ac22974c0de454ad66e6f150c494c7b54611c2027650045c28434"
  },
  "kernelspec": {
   "display_name": "power_perceiver",
   "language": "python",
   "name": "power_perceiver"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
